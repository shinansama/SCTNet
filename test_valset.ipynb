{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/SCTNet/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/root/miniconda3/envs/SCTNet/lib/python3.8/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "import sys\n",
    "import time\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "sys.path.append(current_directory)\n",
    "\n",
    "import mmcv\n",
    "import torch\n",
    "from mmcv.cnn.utils import revert_sync_batchnorm\n",
    "from mmcv.runner import (load_checkpoint,\n",
    "                         wrap_fp16_model)\n",
    "\n",
    "from mmseg.apis import single_gpu_test\n",
    "from mmseg.datasets import build_dataloader, build_dataset\n",
    "from mmseg.models import build_segmentor\n",
    "from mmseg.utils import build_dp, get_device, setup_multi_processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b83dadb0c4d91fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config_path = r\"configs/sctnet/ADE20K/sctnet-b_8x4_160k_ade.py\"\n",
    "# checkpoint = r\"configs/sctnet/ADE20K/pretrain/SCTNet-B-ADE20K.pth\"\n",
    "\n",
    "# config_path = r\"configs/sctnet/COCO-Stuff-10K/sctnet_b_4x4_160k.py\"\n",
    "# checkpoint = r\"configs/sctnet/COCO-Stuff-10K/pretrain/SCTNet-B_COCO-Stuff-10K.pth\"\n",
    "\n",
    "config_path = r\"configs/sctnet/pets/sctnet-b_8x4_160k_pets.py\"\n",
    "checkpoint = r\"work_dirs/sctnet-b_8x4_160k_pets/latest.pth\"\n",
    "\n",
    "cfg = mmcv.Config.fromfile(config_path)\n",
    "\n",
    "# set cudnn_benchmark\n",
    "if cfg.get('cudnn_benchmark', False):\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "cfg.gpu_ids = [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f675df75e2b5bc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mkdir work_dir\n",
    "work_dir = osp.join('./work_dirs', osp.splitext(osp.basename(config_path))[0])\n",
    "mmcv.mkdir_or_exist(osp.abspath(work_dir))\n",
    "timestamp = time.strftime('%Y%m%d_%H%M%S', time.localtime())\n",
    "json_file = osp.join(work_dir, f'eval_single_scale_{timestamp}.json')\n",
    "output_img_dir = osp.join(work_dir, f'eval_single_scale_{timestamp}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "791a1c0c3df45c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-29 08:07:45,363 - mmseg - INFO - Loaded 740 images\n"
     ]
    }
   ],
   "source": [
    "# build the dataloader\n",
    "# TODO: support multiple images per gpu (only minor changes are needed)\n",
    "dataset = build_dataset(cfg.data.test)\n",
    "# The default loader config\n",
    "loader_cfg = dict(\n",
    "    # cfg.gpus will be ignored if distributed\n",
    "    num_gpus=len(cfg.gpu_ids),\n",
    "    dist=False,\n",
    "    shuffle=False)\n",
    "# The overall dataloader settings\n",
    "loader_cfg.update({\n",
    "    k: v\n",
    "    for k, v in cfg.data.items() if k not in [\n",
    "        'train', 'val', 'test', 'train_dataloader', 'val_dataloader',\n",
    "        'test_dataloader'\n",
    "    ]\n",
    "})\n",
    "test_loader_cfg = {\n",
    "    **loader_cfg,\n",
    "    'samples_per_gpu': 1,\n",
    "    'shuffle': False,  # Not shuffle by default\n",
    "    **cfg.data.get('test_dataloader', {})\n",
    "}\n",
    "# build the dataloader\n",
    "data_loader = build_dataloader(dataset, **test_loader_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2480dbe030b67a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-29 08:07:48,435 - mmseg - INFO - initialize SCTNet with init_cfg {'type': 'Pretrained', 'checkpoint': 'pretrain/SCT-B_Pretrain.pth'}\n",
      "2024-11-29 08:07:48,437 - mmcv - INFO - load model from: pretrain/SCT-B_Pretrain.pth\n",
      "2024-11-29 08:07:48,439 - mmcv - INFO - load checkpoint from local path: pretrain/SCT-B_Pretrain.pth\n",
      "/root/workspace/SCTNet/mmseg/models/losses/cross_entropy_loss.py:235: UserWarning: Default ``avg_non_ignore`` is False, if you would like to ignore the certain label and average loss over non-ignore labels, which is the same with PyTorch official cross_entropy, set ``avg_non_ignore=True``.\n",
      "  warnings.warn(\n",
      "2024-11-29 08:07:49,552 - mmseg - INFO - initialize VitGuidanceHead with init_cfg {'type': 'Pretrained', 'checkpoint': 'pretrain/Teacher_SegFormer_B3_ADE.pth'}\n",
      "2024-11-29 08:07:49,553 - mmcv - INFO - load model from: pretrain/Teacher_SegFormer_B3_ADE.pth\n",
      "2024-11-29 08:07:49,553 - mmcv - INFO - load checkpoint from local path: pretrain/Teacher_SegFormer_B3_ADE.pth\n",
      "2024-11-29 08:07:50,243 - mmcv - WARNING - The model and loaded state dict do not match exactly\n",
      "\n",
      "size mismatch for teacher_head.conv_seg.weight: copying a param with shape torch.Size([150, 256, 1, 1]) from checkpoint, the shape in current model is torch.Size([37, 256, 1, 1]).\n",
      "size mismatch for teacher_head.conv_seg.bias: copying a param with shape torch.Size([150]) from checkpoint, the shape in current model is torch.Size([37]).\n",
      "missing keys in source state_dict: feature_transforms.weight, feature_transforms.bias, convstage4.weight, convstage4.bias, convstage3.weight, convstage3.bias\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load checkpoint from local path: work_dirs/sctnet-b_8x4_160k_pets/iter_108000.pth\n"
     ]
    }
   ],
   "source": [
    "# build the model and load checkpoint\n",
    "cfg.model.train_cfg = None\n",
    "model = build_segmentor(cfg.model, test_cfg=cfg.get('test_cfg'))\n",
    "fp16_cfg = cfg.get('fp16', None)\n",
    "if fp16_cfg is not None:\n",
    "    wrap_fp16_model(model)\n",
    "checkpoint = load_checkpoint(model, checkpoint, map_location='cpu')\n",
    "if 'CLASSES' in checkpoint.get('meta', {}):\n",
    "    model.CLASSES = checkpoint['meta']['CLASSES']\n",
    "else:\n",
    "    print('\"CLASSES\" not found in meta, use dataset.CLASSES instead')\n",
    "    model.CLASSES = dataset.CLASSES\n",
    "if 'PALETTE' in checkpoint.get('meta', {}):\n",
    "    model.PALETTE = checkpoint['meta']['PALETTE']\n",
    "else:\n",
    "    print('\"PALETTE\" not found in meta, use dataset.PALETTE instead')\n",
    "    model.PALETTE = dataset.PALETTE\n",
    "\n",
    "# clean gpu memory when starting a new evaluation.\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "cfg.device = get_device()\n",
    "\n",
    "model = revert_sync_batchnorm(model)\n",
    "model = build_dp(model, cfg.device, device_ids=cfg.gpu_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d51de637a6e4926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 740/740, 15.3 task/s, elapsed: 48s, ETA:     0sper class results:\n",
      "\n",
      "+-------+-------+-------+\n",
      "| Class |  IoU  |  Acc  |\n",
      "+-------+-------+-------+\n",
      "|  cat  | 83.02 | 89.72 |\n",
      "|  dog  | 90.59 | 94.15 |\n",
      "+-------+-------+-------+\n",
      "Summary:\n",
      "\n",
      "+-------+------+-------+\n",
      "|  aAcc | mIoU |  mAcc |\n",
      "+-------+------+-------+\n",
      "| 92.65 | 86.8 | 91.93 |\n",
      "+-------+------+-------+\n"
     ]
    }
   ],
   "source": [
    "from mmcv.image import tensor2imgs\n",
    "\n",
    "out_dir = './work_dirs'\n",
    "\n",
    "eval_kwargs = {}\n",
    "\n",
    "model.eval()\n",
    "\n",
    "results = []\n",
    "dataset = data_loader.dataset\n",
    "prog_bar = mmcv.ProgressBar(len(dataset))\n",
    "# The pipeline about how the data_loader retrieval samples from dataset:\n",
    "# sampler -> batch_sampler -> indices\n",
    "# The indices are passed to dataset_fetcher to get data from dataset.\n",
    "# data_fetcher -> collate_fn(dataset[index]) -> data_sample\n",
    "# we use batch_sampler to get correct data idx\n",
    "loader_indices = data_loader.batch_sampler\n",
    "\n",
    "for batch_indices, data in zip(loader_indices, data_loader):\n",
    "    with torch.no_grad():\n",
    "        result = model(return_loss=False, **data)\n",
    "\n",
    "    # draw results\n",
    "    img_tensor = data['img'][0]\n",
    "    img_metas = data['img_metas'][0].data[0]\n",
    "    imgs = tensor2imgs(img_tensor, **img_metas[0]['img_norm_cfg'])\n",
    "    assert len(imgs) == len(img_metas)\n",
    "\n",
    "    if out_dir:\n",
    "        for img, img_meta in zip(imgs, img_metas):\n",
    "            h, w, _ = img_meta['img_shape']\n",
    "            img_show = img[:h, :w, :]\n",
    "\n",
    "            ori_h, ori_w = img_meta['ori_shape'][:-1]\n",
    "            img_show = mmcv.imresize(img_show, (ori_w, ori_h))\n",
    "\n",
    "            if out_dir:\n",
    "                out_file = osp.join(out_dir, img_meta['ori_filename'])\n",
    "            else:\n",
    "                out_file = None\n",
    "\n",
    "            model.module.show_result(\n",
    "                img_show,\n",
    "                result,\n",
    "                palette=dataset.PALETTE,\n",
    "                show=True,\n",
    "                out_file=out_file,\n",
    "                opacity=0.5)\n",
    "\n",
    "    results.extend(result)\n",
    "    batch_size = len(result)\n",
    "    for _ in range(batch_size):\n",
    "        prog_bar.update()\n",
    "\n",
    "eval_kwargs.update(metric=\"mIoU\")\n",
    "metric = dataset.evaluate(results, **eval_kwargs)\n",
    "metric_dict = dict(config=config_path, metric=metric)\n",
    "mmcv.dump(metric_dict, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4ef5fd-a9a2-49d5-b4f2-4db9c9b9237d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SCTNet",
   "language": "python",
   "name": "sctnet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
